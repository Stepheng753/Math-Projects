{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "formatter = {'float': '{: 0.2f}'.format, 'int': '{: 0.2f}'.format}\n",
    "formatter8 = {'float': '{: 0.8f}'.format, 'int': '{: 0.2f}'.format}\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train.csv  : Data Points:  60000  x  784  : Data Labels:  60000  x  10\n",
      "mnist_test.csv  : Data Points:  10000  x  784  : Data Labels:  10000  x  10\n"
     ]
    }
   ],
   "source": [
    "def get_data(file_name):\n",
    "    data_points = []\n",
    "    data_labels = []\n",
    "    with open(file_name, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "\n",
    "\n",
    "        for row in list(reader)[1:]:\n",
    "            label = [0 for _ in range(0, 10)]\n",
    "            label[int(row[0])] = 1\n",
    "            data_points.append([int(x) for x in row[1: ]])\n",
    "            data_labels.append(label)\n",
    "\n",
    "    print(file_name, ' : Data Points: ', len(data_points), ' x ', len(data_points[0]), ' : Data Labels: ', len(data_labels), ' x ', len(data_labels[0]))\n",
    "    return data_points, data_labels\n",
    "\n",
    "\n",
    "training_data_points, training_data_labels = get_data('mnist_train.csv')\n",
    "testing_data_points, testing_data_labels = get_data('mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/ (1 + np.exp(-x))\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return  s * (1 - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_heights: [784, 16, 10]\n",
      "learning_rate: 1\n",
      "weights: (16, 784), (10, 16)\n",
      "biases: (16, 1), (10, 1)\n",
      "\n",
      "Num Cases: 10\n",
      "Layers: (784, 1), (16, 1), (10, 1)\n",
      "Desired_Output: (10, 1)\n",
      "\n",
      "0 :  5.404121199044741\n",
      "0  /  10  -  0.0 %\n",
      "1  :  4.036124954504273\n",
      "0  /  10  -  0.0 %\n",
      "2  :  3.2033503326821218\n",
      "0  /  10  -  0.0 %\n",
      "3  :  2.490385898116353\n",
      "1  /  10  -  10.0 %\n",
      "4  :  2.2129747886737983\n",
      "2  /  10  -  20.0 %\n",
      "5  :  1.9646526356660037\n",
      "Desired:  2 \n",
      " Calculation:  5\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "All Comments will use the following Example:\n",
    "MNIST Number Recognition Project\n",
    "Number of Cases - 60,000\n",
    "Input Layer - 784 Nodes\n",
    "Hidden Layer - 16 Nodes\n",
    "Output Layer - 10 Nodes\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork_v2:\n",
    "\n",
    "    \"\"\"\n",
    "    all_heights       - array    - heights of each layer - [784, 16, 10]\n",
    "    num_layers        - int      - 3\n",
    "    learning_rate     - int      - factor to adjust weights and biases\n",
    "    activation_func   - function - limiter from 0 to 1\n",
    "    d_activation_func - function - derivative of activation\n",
    "    weights           - array    - weights of each layer, which are matrices [W_{layer0}, W_{layer1}]\n",
    "                                 - W_{layer0} = Matrix_{16 x 784}, W_{layer1} = Matrix_{10 x 16}\n",
    "    biases            - array    - biases of each layer, which are matrices [b_{layer0}, b_{layer1}]\n",
    "                                 - b_{layer0} = Matrix_{16 x 784}, b_{layer1} = Matrix_{10 x 16}\n",
    "    \"\"\"\n",
    "    def __init__(self, all_heights, learning_rate = 0.05, activation_func = sigmoid, d_activation_func = d_sigmoid, debug = False):\n",
    "        self.all_heights = all_heights\n",
    "        self.num_layers = len(all_heights)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_func = activation_func\n",
    "        self.d_activation_func = d_activation_func\n",
    "\n",
    "        self.weights = [None for _ in range(0, self.num_layers - 1)]\n",
    "        self.biases = [None for _ in range(0, self.num_layers - 1)]\n",
    "        for height_index in range(0, self.num_layers - 1):\n",
    "            # next_layer_height x prev_layer_height\n",
    "            self.weights[height_index] = np.random.randn(self.all_heights[height_index + 1], self.all_heights[height_index])\n",
    "            # next_layer_height x 1\n",
    "            self.biases[height_index]  = np.random.randn(self.all_heights[height_index + 1], 1)\n",
    "        self.weights = np.array(self.weights)\n",
    "        self.biases = np.array(self.biases)\n",
    "\n",
    "        if debug:\n",
    "            print(self)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    num_cases - int - number of training cases\n",
    "    training_cases - array - each case which contains an array of layers, and its desired output\n",
    "                      - layer - array that contains the values at each layer [784 values, 16 values, 10 values]\n",
    "                      - desired output - array that contains the desired output values [10 values]\n",
    "    input_cases - array 2D - each row is one case of training data\n",
    "    desired_output_cases - each row is one desired_output of each case of training data\n",
    "\n",
    "    \"\"\"\n",
    "    def init_all_layers(self, input_cases, desired_output_cases, debug = False):\n",
    "        self.num_cases = len(input_cases)\n",
    "        self.training_cases = [None for _ in range(0, self.num_cases)]\n",
    "\n",
    "        # Each Case, initialize each case's input layer then forward prop it\n",
    "        for case_index in range(0, self.num_cases):\n",
    "            input_layer = np.array([input_cases[case_index]]).T\n",
    "            correct_output = np.array([desired_output_cases[case_index]]).T\n",
    "            self.training_cases[case_index] = {'Layers' : [None for _ in range(0, self.num_layers)],\n",
    "                                               'Desired_Output' : correct_output}\n",
    "\n",
    "            self.training_cases[case_index]['Layers'][0] = input_layer\n",
    "            self.training_cases = self.forward_prop(self.training_cases, case_index, debug)\n",
    "\n",
    "        if debug:\n",
    "            num_cases_str = 'Num Cases: ' + str(self.num_cases) + '\\n'\n",
    "\n",
    "            first_layer = self.training_cases[0]['Layers']\n",
    "            layer_str = 'Layers: '\n",
    "            for layer_index in range(0, len(first_layer) - 1):\n",
    "                layer_str += str(first_layer[layer_index].shape) + ', '\n",
    "            layer_str += str(first_layer[len(first_layer) - 1].shape) + '\\n'\n",
    "\n",
    "            output_str = 'Desired_Output: ' + str(self.training_cases[0]['Desired_Output'].shape) + '\\n'\n",
    "\n",
    "            print(num_cases_str + layer_str + output_str)\n",
    "\n",
    "\n",
    "    def train(self, input_cases, correct_output_cases, iterations, debug = False):\n",
    "        self.init_all_layers(input_cases, correct_output_cases, debug)\n",
    "        if debug:\n",
    "            print('0 : ', self.calculate_error(False))\n",
    "        for i in range(0, iterations):\n",
    "            self.update_NN(False)\n",
    "            if debug:\n",
    "                print(i + 1, ' : ', self.calculate_error(False))\n",
    "\n",
    "\n",
    "    def forward_prop(self, cases, case_index, debug = False):\n",
    "        layers = cases[case_index]['Layers']\n",
    "        for layer_index in range(0, len(self.weights)):\n",
    "            layers[layer_index + 1] = self.activation_func(self.weights[layer_index].dot(layers[layer_index]) + self.biases[layer_index])\n",
    "        cases[case_index]['Layers'] = layers\n",
    "\n",
    "        return cases\n",
    "\n",
    "\n",
    "    def backward_prop(self, case_index, debug = False):\n",
    "        curr_case = self.training_cases[case_index]\n",
    "        curr_case_layers = curr_case['Layers']\n",
    "        curr_case_desired_output = curr_case['Desired_Output']\n",
    "\n",
    "        d_weights = [None for _ in range(0, len(self.weights))]\n",
    "        d_biases = [None for _ in range(0, len(self.biases))]\n",
    "\n",
    "        d_outer = 2 * (curr_case_layers[self.num_layers - 1] - curr_case_desired_output)\n",
    "\n",
    "        d_activations = []\n",
    "        for layer_index in range(self.num_layers - 2, -1, -1):\n",
    "            d_activations.append(self.d_activation_func(self.weights[layer_index].dot(curr_case_layers[layer_index]) + self.biases[layer_index]))\n",
    "\n",
    "        for layer_index in range(self.num_layers - 2, -1, -1):\n",
    "            weights_v_layers = []\n",
    "            for dec_layer_index in range(self.num_layers - 2, layer_index, -1):\n",
    "                weights_v_layers.append(self.weights[dec_layer_index])\n",
    "            weights_v_layers.append(curr_case_layers[layer_index])\n",
    "\n",
    "            d_weight = d_outer\n",
    "            d_bias = 0\n",
    "            for weights_v_layers_index in range(0, len(weights_v_layers)):\n",
    "                d_weight *= d_activations[weights_v_layers_index]\n",
    "                if weights_v_layers_index == len(weights_v_layers) - 1:\n",
    "                    d_bias = d_weight\n",
    "                    d_weight = d_weight.dot(weights_v_layers[weights_v_layers_index].T)\n",
    "                else:\n",
    "                    d_weight = weights_v_layers[weights_v_layers_index].T.dot(d_weight)\n",
    "\n",
    "            d_weights[layer_index] = self.learning_rate * d_weight\n",
    "            d_biases[layer_index] = self.learning_rate * d_bias\n",
    "\n",
    "        return d_weights, d_biases\n",
    "\n",
    "    def update_NN(self, debug = False):\n",
    "        d_weights, d_biases = self.backward_prop(0, debug)\n",
    "        average_quotient = 1 / self.num_cases\n",
    "        for case_index in range(1, self.num_cases):\n",
    "            add_weights, add_biases = self.backward_prop(case_index, debug)\n",
    "            for layer in range(0, len(add_weights)):\n",
    "                d_weights[layer] += average_quotient * add_weights[layer]\n",
    "                d_biases[layer] += average_quotient * add_biases[layer]\n",
    "\n",
    "        for layer in range(0, len(add_weights)):\n",
    "            self.weights[layer] -= d_weights[layer]\n",
    "            self.biases[layer] -= d_biases[layer]\n",
    "\n",
    "\n",
    "        correct = 0\n",
    "        for case_index in range(0, self.num_cases):\n",
    "            self.training_cases = self.forward_prop(self.training_cases, case_index, debug)\n",
    "            if np.argmax(self.training_cases[case_index]['Layers'][self.num_layers - 1]) == np.argmax(self.training_cases[case_index]['Desired_Output']):\n",
    "                correct += 1\n",
    "        print(correct, ' / ', self.num_cases, ' - ', 100 * correct / self.num_cases, '%')\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    return sum( (calculated_output - desired_output)^2 )\n",
    "    \"\"\"\n",
    "    def calculate_error_per_case(self, case_index, debug = False):\n",
    "        calculated_output = self.training_cases[case_index]['Layers'][self.num_layers - 1]\n",
    "        desired_output = self.training_cases[case_index]['Desired_Output']\n",
    "        diff = calculated_output - desired_output\n",
    "        diff_sq = diff ** 2\n",
    "        error = np.sum(diff_sq)\n",
    "\n",
    "        if debug:\n",
    "            print('Calculated Output: \\n' , calculated_output)\n",
    "            print('Desired Output: \\n' , desired_output)\n",
    "            print('Error: ', error)\n",
    "\n",
    "        return error\n",
    "\n",
    "\n",
    "    def calculate_error(self, debug = False):\n",
    "        error = 0\n",
    "        for case_index in range(0, self.num_cases):\n",
    "            error += self.calculate_error_per_case(case_index, debug)\n",
    "        return error / self.num_cases\n",
    "\n",
    "\n",
    "    def test(self, input_cases, correct_output_cases, testing_indices, debug = False):\n",
    "        num_cases = len(input_cases)\n",
    "        testing_cases = [None for _ in range(0, num_cases)]\n",
    "        for case_index in range(0, num_cases):\n",
    "            input_layer = np.array([input_cases[case_index]]).T\n",
    "            correct_output = np.array([correct_output_cases[case_index]]).T\n",
    "            testing_cases[case_index] = {'Layers' : [None for _ in range(0, self.num_layers)],\n",
    "                                         'Desired_Output' : correct_output}\n",
    "            testing_cases[case_index]['Layers'][0] = input_layer\n",
    "\n",
    "        for test_index in testing_indices:\n",
    "            testing_cases = self.forward_prop(testing_cases, test_index)\n",
    "            print('Desired: ', np.argmax(testing_cases[test_index]['Desired_Output']), '\\n',\n",
    "                  'Calculation: ', np.argmax(testing_cases[test_index]['Layers'][self.num_layers - 1]))\n",
    "\n",
    "    def __str__(self):\n",
    "\n",
    "        all_heights_str = 'all_heights: ' + str(self.all_heights) + '\\n'\n",
    "        learning_rate_str = 'learning_rate: ' + str(self.learning_rate) + '\\n'\n",
    "\n",
    "        weight_str = 'weights: '\n",
    "        bias_str = 'biases: '\n",
    "        for layer_index in range(0, len(self.weights) - 1):\n",
    "            weight_str += str(self.weights[layer_index].shape) + ', '\n",
    "            bias_str += str(self.biases[layer_index].shape)    + ', '\n",
    "        weight_str += str(self.weights[len(self.weights) - 1].shape) + '\\n'\n",
    "        bias_str += str(self.biases[len(self.weights) - 1].shape)    + '\\n'\n",
    "\n",
    "        return all_heights_str + learning_rate_str + weight_str + bias_str\n",
    "\n",
    "\n",
    "NN_v2 = NeuralNetwork_v2([784, 16, 10], learning_rate=1, debug=True)\n",
    "NN_v2.train(training_data_points[0 : 10], training_data_labels[0 : 10], 5, debug=True)\n",
    "NN_v2.test(testing_data_points, testing_data_labels, [1], debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fcb85a27ab5198b72a1c9f56de6bda92f51d1598fc2ef39379274235c7c516bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
